{"cells":[{"cell_type":"markdown","metadata":{"id":"QcJK3kXl--c3"},"source":["# CS354 Assignment 2: PyTorch 101\n","\n","Before we start, please put your name in following format:\n","\n","FirstName Lastname, netid//   e.g.) Safwan Wshah, swshah"]},{"cell_type":"markdown","metadata":{"id":"1NizYwbfjXUg"},"source":["To run your code:\n","\n","*   Save your code to google drive, right click and open using google colab.\n","*   Under google drive root directory create the following folder and make sure all your assignments are under this directory (CS354-Assignments-2022/yournetid-A#). For example, for assignment #1 it should look like this \n","CS354-Assignments-2022/swshah-A1\n","\n","\n","\n","To submit, please follow the following steps:\n","*   Zip your assignment and name it yournetid-A#.zip For example for assignment 1, it should be swshah-A1.zip.\n","* Submit to blackboard. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"sI2nCGLOlit2"},"source":["# Self Grading\n","How many cells you got their outputs similar to what expected.\n","\n","(Same Expected output cell/#totoal number of cell) x 100 = "]},{"cell_type":"markdown","metadata":{"id":"kQndOAmiVTO3"},"source":["# Setup Code\n","Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n","\n","First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"H5PzjwH7VTO4","executionInfo":{"status":"ok","timestamp":1645637240098,"user_tz":300,"elapsed":12,"user":{"displayName":"safwan wshah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17703726182199816036"}}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"bCtoiSyVVTO8"},"source":["### Google Colab Setup\n","Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"tHG0slB6VTO8","executionInfo":{"status":"ok","timestamp":1645637259283,"user_tz":300,"elapsed":19195,"user":{"displayName":"safwan wshah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17703726182199816036"}},"outputId":"dfd7d5e6-96f9-40f2-921a-fed09b2af3ee","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"markdown","metadata":{"id":"UWjXo-vXVTO_"},"source":["Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames in the assignment directory"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"KqMvJnNHVTPA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645637261535,"user_tz":300,"elapsed":853,"user":{"displayName":"safwan wshah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17703726182199816036"}},"outputId":"1ecda4e2-6510-42d9-e68a-5be609c1d297"},"outputs":[{"output_type":"stream","name":"stdout","text":["['images', '__pycache__', 'mobilenet_solutions.ipynb', 'mobilenet.ipynb']\n"]}],"source":["import os\n","import sys\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment, it should be under CS354-Assignments-2022/netid-A1\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'CS354-Assignments-2022/swshah-A2' # change this directory to yours\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Eastern\"\n","time.tzset()"]},{"cell_type":"markdown","source":["# **Let us start the assignment - Introduction**\n","\n","\n","\n","**(Highly recommeneded)** Briefly read Google's 2017 paper mobilenets: \n","\n","[Efficient revolutionary neural networks for mobile vision applications to experience Depthwise convolution and Pointwise convolution.](https://arxiv.org/pdf/1704.04861.pdf)\n","\n","The MobileNet model is based on depthwise separable\n","convolutions which is a form of **factorized convolutions**\n","which factorize a standard convolution into a **depthwise convolution** and a **1×1 convolution** called a pointwise convolution. For MobileNets the depthwise convolution applies a **single filter** to each input channel.\n","\n","The pointwise convolution then applies a 1×1 convolution to combine the\n","outputs the depthwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. The depthwise separable convolution splits this into two layers, a separate layer for filtering and a separate layer for combining. This **factorization** has the effect of drastically **reducing computation and model size**. Figure below shows how a standard convolution (a) is factorized into a depthwise convolution (b) and a 1 × 1 pointwise convolution (c).\n","Figure 1. shows standard, depthwise and pointwise convolutional filters.\n","<center>\n","\n","![picture](https://drive.google.com/uc?id=1JT9XTPV_34juMeS8gJHzltXfFlXthnNb)\n","\n","**Figure (1)**\n","\n","</center>\n","\n","\n","A standard convolutional layer takes as input a $D_F$ × $D_F$ × M feature map F and produces a $D_G$ × $D_G$ × N.  feature map G where $D_F$ is the spatial width and height of a square input feature map, M is the number of input channels (input depth), $D_G$ is the spatial width and height of a square output feature map and N is the number of output channel (output depth).\n","\n","Depthwise Separable convolution are made up of two layers: depthwise convolutions and pointwise convolutions. Depthwise convolutions to apply a **single filter per each input channel (input depth)**. Pointwise convolution, a\n","**simple 1×1 convolution, is then used to create a linear combination of the output of the depthwise layers**. MobileNets use both batchnorm and ReLU nonlinearities for both layers.\n","\n","<center>\n","\n","![picture](https://drive.google.com/uc?id=1a7oXRlGUU6iwd16-xO7gH8ZbE-LDFsXw)\n","\n","**Figure (2): Left: shows standard convolutional layer with batchnorm and ReLU. Right: Depthwise Separable convolutions with Depthwise and Pointwise layers followed by batchnorm and ReLU.**\n","\n","</center>\n","\n","\n","\n","\n"],"metadata":{"id":"nXDTbgetJPt5"}},{"cell_type":"code","source":["#check torch version\n","import torch\n","print(torch.__version__)"],"metadata":{"id":"XeS7AZ1xo7YA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645637278930,"user_tz":300,"elapsed":6194,"user":{"displayName":"safwan wshah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17703726182199816036"}},"outputId":"453c46f2-77dc-4f51-f135-3a4fda4fa04f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n"]}]},{"cell_type":"markdown","source":["#MobileNet separable convolution\n","\n","In this cell, you are asked to implement both Mobilenet and Standerd block. showed in the image below. \n","\n","![picture](https://drive.google.com/uc?id=1a7oXRlGUU6iwd16-xO7gH8ZbE-LDFsXw)\n","\n","Remember that 3x3 Depthwise Conv generate same number of feature maps equal to the input channels (depth). Each of these feature maps is a result of a convolution operation over each input channel separately. The 1x1 convolution is the typical 1x1 convolution. Note that doing it this way it will kind of  equivalent to the standard convolution but with less number of parameters and calculations.\n","\n","To illustrate this: \n","\n","Suppose that you have the 3x3 standard colv on WxHxD volume and the output is WxHxD. \n","- The number of parameters will be: 3x3xDxD \n","- The number of conv operations: WxHxD each at 3x3xD\n","\n","For the mobilenet to implement equivalent 3x3 to the standard convolutions on same input and output:\n","\n","- The number of parameters will be: 3x3xD + 1x1xDxD \n","- The number of conv operations: WxHxD each at 3x3 + WxHxD each at 1x1xD.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"gEXjnia3p47x"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.optim as optim\n","\n","class MobileNetBlock(nn.Module):\n","    def __init__(self, in_depth, out_depth, stride=1):\n","        super(MobileNetBlock, self).__init__()\n","        # add your code here ... \n","        # hint: For Depthwise convolution, use the Conv2d with groups equal to the input depth\n","\n","    def forward(self, x):\n","        # add your code here\n","\n","        return out\n","\n","\n","class StandardBlock(nn.Module):\n","    def __init__(self, in_depth, out_depth, stride=1):\n","        super(StandardBlock, self).__init__()\n","        # add your code here ... \n","\n","    def forward(self, x):\n","        # add your code here\n","        return out"],"metadata":{"id":"XahD_x5skiT2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Create DataLoader\n","Now, you are asked to implement dataloaders that will be used during the training and testing to load the data. To implement the dataloaders you need to implement the following:\n","- We will work on CIFAR dataset. I am providing few lines to start from.\n","- transform_train and transform_test: Used to transoform and normalize your data. remember that you need to normalize the data. Do you need to use transformation in the test set ? Justify your answer.\n","- trainloader, test loader: use batch size of 128.\n"],"metadata":{"id":"7CJT0KduxnkT"}},{"cell_type":"code","source":["trainset = torchvision.datasets.CIFAR10(root='./data', train=True,  download=True)\n","testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n","\n","transform_train = # add your  code here\n","transform_test = # add your  code here\n","\n","# add transformation to your data\n","\n","trainloader = # add your  code here\n","testloader = # add your  code here"],"metadata":{"id":"ajbQ5_11OD48","colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["c5c26fc50da54b62bc4783db929178d4","c5efd26cdaa440a5a0eff9cacc7e3c45","2cc53666da7a415ea437bf289132610a","7c1749f00fd34384b612d9eff4c72440","7beb1f0b3bbc467698ae3aa57c91f7d6","8c9c4877891346af8a22fb685a378190","2936b5678ff14674b8229ae713936fac","db81a0055ddd4502a53fedba41233da6","f762c8430ca749e38b5f4bdb1f56e857","a50520824a094d00b95dfe063c6020a5","a536ee96e15a4293a3f3141d558afa63"]},"executionInfo":{"status":"ok","timestamp":1645637323696,"user_tz":300,"elapsed":13351,"user":{"displayName":"safwan wshah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17703726182199816036"}},"outputId":"07be4543-c3da-4619-917c-587fedcee3ad"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5c26fc50da54b62bc4783db929178d4","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Data mean [0.49139968 0.48215841 0.44653091]\n","Data std [0.24703223 0.24348513 0.26158784]\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["#Create MobileNetV1 network\n","\n","Now it is the time to create MobileNet network that will classify CIFAR dataset. You will be asked to implement the following architecture. All the convolutions should use the MobileNetBlock except the first convolutional layer which should be standard one (We are trying to follow same steps in the MobileNet paper by starting with standard convolution). \n","\n","- 32×32×3 ==> (Standerd convoltion)\n","- 32×32×32 ==> (MobileNet convoltion)\n","- 32×32×64 ==> (MobileNet convoltion)\n","- 16×16×128 ==> (MobileNet convoltion)\n","- 16×16×128 ==>(MobileNet convoltion)\n","- 8×8×256 ==>(MobileNet convoltion)\n","- 8×8×256 ==>(MobileNet convoltion)\n","- 4×4×512 ==>(MobileNet convoltion)\n","- 4×4×512 ==>(MobileNet convoltion)\n","- 2×2×1024 ==>(MobileNet convoltion)\n","- 2×2×1024\n","- Mean pooling ==> 1 × 1 × 1024\n","- Finally, it is fully connected to 10 output nodes\n","\n"],"metadata":{"id":"3fHRRpLmqJgy"}},{"cell_type":"code","source":["class MobileNetV1(nn.Module):\n","    \n","    def __init__(self, num_classes=10):\n","        super(MobileNetV1, self).__init__()\n","        # add your code here \n","    \n","    def forward(self, x):\n","        # add your code here \n","        return out"],"metadata":{"id":"kGhiIHebl_cv","executionInfo":{"status":"ok","timestamp":1645637344659,"user_tz":300,"elapsed":2,"user":{"displayName":"safwan wshah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17703726182199816036"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Now, Implement the StandardNet that replace all MobileNet blocks with StanderdBlocks. "],"metadata":{"id":"SiWybsOVWlVH"}},{"cell_type":"code","source":["class StandardNet(nn.Module):\n","    \n","    def __init__(self, num_classes=10):\n","        super(StandardNet, self).__init__()\n","        # add your code here \n","\n","    def forward(self, x):\n","        # add your code here \n","        return out"],"metadata":{"id":"NYUAVaFTmNdR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Instantiation network\n","\n","- Use GPU for faster training.\n","- Define two networks, MobileNet and stNet. \n","- Move these networks to the GPU.\n","- use an appropriate loss functions. \n","- use an appropriate optimization functions.\n"],"metadata":{"id":"WmNtswBjqoIU"}},{"cell_type":"code","source":["# To use GPU, you can set it in the menu \"Runtime\" - > \"change runtime type\"\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Put the network on the GPU\n","stNet = # add your code here for StandardNet, remeber to move the network for GPU\n","MobileNet = # add your code here\n","criterion = # add your code here to define the loss function\n","optimizer = # add your code here to define the optimizer.\n","MobileNetoptimizer = # add your code here to define the optimizer."],"metadata":{"id":"rcM4XxgBmj3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#model training\n","\n","Now put everthing togather and train/test the two networks in the cells below. Train only for 10 epochs for each model.\n"],"metadata":{"id":"Q56YKibUqwAL"}},{"cell_type":"code","source":["# train MobileNet here.\n","for epoch in range(10):  # Repeat multiple rounds\n","    for i, (inputs, labels) in enumerate(trainloader):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        \n","        # add your code here ... \n","        # Output statistics\n","        if i % 100 == 0:   \n","            print('Epoch: %d Minibatch: %5d loss: %.3f' %(epoch + 1, i + 1, loss.item()))\n","\n","print('Finished Training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VyYlyy2cqsin","executionInfo":{"status":"ok","timestamp":1645637644945,"user_tz":300,"elapsed":159063,"user":{"displayName":"safwan wshah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17703726182199816036"}},"outputId":"0fe84aa3-1c9c-4907-b05d-84c68a7d58ba"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 Minibatch:     1 loss: 2.344\n","Epoch: 1 Minibatch:   101 loss: 1.870\n","Epoch: 1 Minibatch:   201 loss: 1.661\n","Epoch: 1 Minibatch:   301 loss: 1.362\n","Epoch: 2 Minibatch:     1 loss: 1.329\n","Epoch: 2 Minibatch:   101 loss: 1.426\n","Epoch: 2 Minibatch:   201 loss: 1.151\n","Epoch: 2 Minibatch:   301 loss: 1.229\n","Epoch: 3 Minibatch:     1 loss: 1.206\n","Epoch: 3 Minibatch:   101 loss: 1.020\n","Epoch: 3 Minibatch:   201 loss: 0.911\n","Epoch: 3 Minibatch:   301 loss: 1.136\n","Epoch: 4 Minibatch:     1 loss: 1.148\n","Epoch: 4 Minibatch:   101 loss: 0.903\n","Epoch: 4 Minibatch:   201 loss: 0.807\n","Epoch: 4 Minibatch:   301 loss: 0.897\n","Epoch: 5 Minibatch:     1 loss: 0.763\n","Epoch: 5 Minibatch:   101 loss: 0.777\n","Epoch: 5 Minibatch:   201 loss: 0.836\n","Epoch: 5 Minibatch:   301 loss: 0.906\n","Epoch: 6 Minibatch:     1 loss: 0.778\n","Epoch: 6 Minibatch:   101 loss: 0.821\n","Epoch: 6 Minibatch:   201 loss: 0.771\n","Epoch: 6 Minibatch:   301 loss: 0.609\n","Epoch: 7 Minibatch:     1 loss: 0.539\n","Epoch: 7 Minibatch:   101 loss: 0.662\n","Epoch: 7 Minibatch:   201 loss: 0.867\n","Epoch: 7 Minibatch:   301 loss: 0.575\n","Epoch: 8 Minibatch:     1 loss: 0.548\n","Epoch: 8 Minibatch:   101 loss: 0.581\n","Epoch: 8 Minibatch:   201 loss: 0.663\n","Epoch: 8 Minibatch:   301 loss: 0.620\n","Epoch: 9 Minibatch:     1 loss: 0.693\n","Epoch: 9 Minibatch:   101 loss: 0.585\n","Epoch: 9 Minibatch:   201 loss: 0.661\n","Epoch: 9 Minibatch:   301 loss: 0.557\n","Epoch: 10 Minibatch:     1 loss: 0.560\n","Epoch: 10 Minibatch:   101 loss: 0.384\n","Epoch: 10 Minibatch:   201 loss: 0.627\n","Epoch: 10 Minibatch:   301 loss: 0.564\n","Finished Training\n"]}]},{"cell_type":"markdown","source":["#Model test  "],"metadata":{"id":"xWHnpAtIq4nH"}},{"cell_type":"code","source":["# Test MobileNet here ...\n","correct = 0\n","total = 0\n","\n","for data in testloader:\n","    # add your code here ...\n","\n","print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n","    100 * correct / total))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0v8QtrMrqzVR","executionInfo":{"status":"ok","timestamp":1645637713459,"user_tz":300,"elapsed":9181,"user":{"displayName":"safwan wshah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17703726182199816036"}},"outputId":"4d331f85-4c73-451c-9df6-340f1e67743e"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the network on the 10000 test images: 82.35 %\n"]}]},{"cell_type":"code","source":["# Train StandardNet here "],"metadata":{"id":"yeRORBEUcjWq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test StandardNet here "],"metadata":{"id":"iw_VhKe1cjpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Parameters \n","Print the summary for each net here. "],"metadata":{"id":"fIeWSvSjcy6A"}},{"cell_type":"code","source":["from torchsummary import summary\n","\n","summary(stNet, (3,32,32), device = 'cuda')\n","summary(MobileNet, (3,32,32), device = 'cuda')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ff9D58vq7Fh","executionInfo":{"status":"ok","timestamp":1645634001302,"user_tz":300,"elapsed":144,"user":{"displayName":"safwan wshah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17703726182199816036"}},"outputId":"e9e1d73b-9b0b-4eac-9f03-081e1763c6f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 32, 32]             864\n","       BatchNorm2d-2           [-1, 32, 32, 32]              64\n","            Conv2d-3           [-1, 64, 32, 32]          18,432\n","       BatchNorm2d-4           [-1, 64, 32, 32]             128\n","     StandardBlock-5           [-1, 64, 32, 32]               0\n","            Conv2d-6          [-1, 128, 16, 16]          73,728\n","       BatchNorm2d-7          [-1, 128, 16, 16]             256\n","     StandardBlock-8          [-1, 128, 16, 16]               0\n","            Conv2d-9          [-1, 128, 16, 16]         147,456\n","      BatchNorm2d-10          [-1, 128, 16, 16]             256\n","    StandardBlock-11          [-1, 128, 16, 16]               0\n","           Conv2d-12            [-1, 256, 8, 8]         294,912\n","      BatchNorm2d-13            [-1, 256, 8, 8]             512\n","    StandardBlock-14            [-1, 256, 8, 8]               0\n","           Conv2d-15            [-1, 256, 8, 8]         589,824\n","      BatchNorm2d-16            [-1, 256, 8, 8]             512\n","    StandardBlock-17            [-1, 256, 8, 8]               0\n","           Conv2d-18            [-1, 512, 4, 4]       1,179,648\n","      BatchNorm2d-19            [-1, 512, 4, 4]           1,024\n","    StandardBlock-20            [-1, 512, 4, 4]               0\n","           Conv2d-21            [-1, 512, 4, 4]       2,359,296\n","      BatchNorm2d-22            [-1, 512, 4, 4]           1,024\n","    StandardBlock-23            [-1, 512, 4, 4]               0\n","           Conv2d-24           [-1, 1024, 2, 2]       4,718,592\n","      BatchNorm2d-25           [-1, 1024, 2, 2]           2,048\n","    StandardBlock-26           [-1, 1024, 2, 2]               0\n","           Conv2d-27           [-1, 1024, 2, 2]       9,437,184\n","      BatchNorm2d-28           [-1, 1024, 2, 2]           2,048\n","    StandardBlock-29           [-1, 1024, 2, 2]               0\n","           Linear-30                   [-1, 10]          10,250\n","================================================================\n","Total params: 18,838,058\n","Trainable params: 18,838,058\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 4.81\n","Params size (MB): 71.86\n","Estimated Total Size (MB): 76.69\n","----------------------------------------------------------------\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 32, 32]             864\n","       BatchNorm2d-2           [-1, 32, 32, 32]              64\n","            Conv2d-3           [-1, 32, 32, 32]             288\n","       BatchNorm2d-4           [-1, 32, 32, 32]              64\n","            Conv2d-5           [-1, 64, 32, 32]           2,048\n","       BatchNorm2d-6           [-1, 64, 32, 32]             128\n","    MobileNetBlock-7           [-1, 64, 32, 32]               0\n","            Conv2d-8           [-1, 64, 16, 16]             576\n","       BatchNorm2d-9           [-1, 64, 16, 16]             128\n","           Conv2d-10          [-1, 128, 16, 16]           8,192\n","      BatchNorm2d-11          [-1, 128, 16, 16]             256\n","   MobileNetBlock-12          [-1, 128, 16, 16]               0\n","           Conv2d-13          [-1, 128, 16, 16]           1,152\n","      BatchNorm2d-14          [-1, 128, 16, 16]             256\n","           Conv2d-15          [-1, 128, 16, 16]          16,384\n","      BatchNorm2d-16          [-1, 128, 16, 16]             256\n","   MobileNetBlock-17          [-1, 128, 16, 16]               0\n","           Conv2d-18            [-1, 128, 8, 8]           1,152\n","      BatchNorm2d-19            [-1, 128, 8, 8]             256\n","           Conv2d-20            [-1, 256, 8, 8]          32,768\n","      BatchNorm2d-21            [-1, 256, 8, 8]             512\n","   MobileNetBlock-22            [-1, 256, 8, 8]               0\n","           Conv2d-23            [-1, 256, 8, 8]           2,304\n","      BatchNorm2d-24            [-1, 256, 8, 8]             512\n","           Conv2d-25            [-1, 256, 8, 8]          65,536\n","      BatchNorm2d-26            [-1, 256, 8, 8]             512\n","   MobileNetBlock-27            [-1, 256, 8, 8]               0\n","           Conv2d-28            [-1, 256, 4, 4]           2,304\n","      BatchNorm2d-29            [-1, 256, 4, 4]             512\n","           Conv2d-30            [-1, 512, 4, 4]         131,072\n","      BatchNorm2d-31            [-1, 512, 4, 4]           1,024\n","   MobileNetBlock-32            [-1, 512, 4, 4]               0\n","           Conv2d-33            [-1, 512, 4, 4]           4,608\n","      BatchNorm2d-34            [-1, 512, 4, 4]           1,024\n","           Conv2d-35            [-1, 512, 4, 4]         262,144\n","      BatchNorm2d-36            [-1, 512, 4, 4]           1,024\n","   MobileNetBlock-37            [-1, 512, 4, 4]               0\n","           Conv2d-38            [-1, 512, 2, 2]           4,608\n","      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n","           Conv2d-40           [-1, 1024, 2, 2]         524,288\n","      BatchNorm2d-41           [-1, 1024, 2, 2]           2,048\n","   MobileNetBlock-42           [-1, 1024, 2, 2]               0\n","           Conv2d-43           [-1, 1024, 2, 2]           9,216\n","      BatchNorm2d-44           [-1, 1024, 2, 2]           2,048\n","           Conv2d-45           [-1, 1024, 2, 2]       1,048,576\n","      BatchNorm2d-46           [-1, 1024, 2, 2]           2,048\n","   MobileNetBlock-47           [-1, 1024, 2, 2]               0\n","           Linear-48                   [-1, 10]          10,250\n","================================================================\n","Total params: 2,142,026\n","Trainable params: 2,142,026\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 6.72\n","Params size (MB): 8.17\n","Estimated Total Size (MB): 14.90\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["- Comment on the performance vs number of paramters vs number of calculations (speed) for each network here. \n","- What are the Pros and Cons for each Net?\n"],"metadata":{"id":"BND82EDhdEcs"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"mobilenet.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c5c26fc50da54b62bc4783db929178d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c5efd26cdaa440a5a0eff9cacc7e3c45","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2cc53666da7a415ea437bf289132610a","IPY_MODEL_7c1749f00fd34384b612d9eff4c72440","IPY_MODEL_7beb1f0b3bbc467698ae3aa57c91f7d6"]}},"c5efd26cdaa440a5a0eff9cacc7e3c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2cc53666da7a415ea437bf289132610a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8c9c4877891346af8a22fb685a378190","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2936b5678ff14674b8229ae713936fac"}},"7c1749f00fd34384b612d9eff4c72440":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_db81a0055ddd4502a53fedba41233da6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f762c8430ca749e38b5f4bdb1f56e857"}},"7beb1f0b3bbc467698ae3aa57c91f7d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a50520824a094d00b95dfe063c6020a5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:06&lt;00:00, 31091977.62it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a536ee96e15a4293a3f3141d558afa63"}},"8c9c4877891346af8a22fb685a378190":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2936b5678ff14674b8229ae713936fac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db81a0055ddd4502a53fedba41233da6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f762c8430ca749e38b5f4bdb1f56e857":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a50520824a094d00b95dfe063c6020a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a536ee96e15a4293a3f3141d558afa63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}